---
title: "House Price Advanced Regression Hyper Parameters XGBoost"
author: "이춘호"
date: "2017년 10월 28일"
output: html_document
---

---
## 1. 데이터 준비
```{r}
x.train <- read.csv("C:/Users/bevis/Downloads/House_Prices_Advanced_Regression_Techniques/train_eda.csv")

x.test <- read.csv("C:/Users/bevis/Downloads/House_Prices_Advanced_Regression_Techniques/test_eda.csv")

x.train <- x.train[, -1]
x.test <- x.test[, -1]

if (!require("dplyr")) { install.packages("dplyr") }

SalePrice <- x.train[, 82]

train_df_model <- x.train[, 1:81]
test_df <- x.test
test_id <- c(1461:2919)

train_test <- bind_rows(train_df_model, test_df) ## 합치면서 character 컬럼이 생김
ntrain <- nrow(train_df_model)

features <- names(x.train)

#convert character into factor : character 컬럼 factor 수정
for (f in features) {
	if (is.character(train_test[[f]])) {
		levels = sort(unique(train_test[[f]]))
		train_test[[f]] = as.integer(factor(train_test[[f]], levels = levels))
	}
}

#splitting whole data back again
train_x <- train_test %>% .[1:ntrain,]
test_x <- train_test %>% .[(ntrain + 1):nrow(train_test),]

train_xy <- cbind(train_x, SalePrice)

if (!require("caret")) { install.packages("caret") }
set.seed(222)

inTrain <- createDataPartition(y = train_xy$SalePrice, p = 0.7, list = FALSE)
Training_xy <- train_xy[inTrain,]
Validation_xy <- train_xy[-inTrain,]

train_test_double <- train_test

for (f in features) {
	if (is.factor(train_test_double[[f]])) {
		levels = sort(unique(train_test_double[[f]]))
		train_test_double[[f]] = as.numeric(train_test_double[[f]], levels = levels)
	}
}

#splitting whole data back again
X_train <- train_test_double %>% .[1:ntrain,]
X_test <- train_test_double %>% .[(ntrain + 1):nrow(train_test_double),]

train_XY <- cbind(X_train, SalePrice)

if (!require("caret")) { install.packages("caret") }
set.seed(222)

inTrain <- createDataPartition(y = train_XY$SalePrice, p = 0.7, list = FALSE)
Training_XY <- train_XY[inTrain,]
Validation_XY <- train_XY[-inTrain,]
```

##  2. XGBoost Hyper Parameters

```{r}
if (!require("xgboost")) { install.packages("xgboost") }
if (!require("Metrics")) { install.packages("Metrics") }

dtrain <- xgb.DMatrix(as.matrix(Training_XY[,-82]), label = Training_XY$SalePrice)
dtest <- xgb.DMatrix(as.matrix(X_test))
```

* 참고 - https://datascience.stackexchange.com/questions/9364/hypertuning-xgboost-parameters

#### 초기에는 구간을 간단하게 설정하여, 세분화할 구간을 선택
#### 범위가 세분화 될 수록 searchGridSubCol 값이 커지므로, 초반에는 범위를 넓게하여 차츰 좁히는 방식 추천 

* 간단하게 설정
```{r}
searchGridSubCol1 <- expand.grid(subsample = seq(0.5, 0.6, by = 0.1),
								colsample_bytree = 0.7,
								ntrees = 2800,
								min_child_weight = seq(1.5, 2, by = 0.1),
								gamma = seq(0.0, 0.1, by = 0.01))

head(searchGridSubCol1)
```

* 결과

```{r}
##       rmse currentSubsampleRate currentColsampleRate ntreesNum
## 1 21359.60                  0.5                  0.5      2000
## 2 21301.15                  0.7                  0.5      2000
## 3 20943.72                  0.5                  0.6      2000
## 4 20935.70                  0.6                  0.6      2000
## 5 20824.64                  0.5                  0.6      2600
## 6 20791.38                  0.5                  0.6      3000
## 7 20758.49                  0.5                  0.6      2200
## 8 20735.79                  0.5                  0.7      2400
## 9 20728.98                  0.6                  0.7      2800
```

* 세분화하여 설정

```{r}
searchGridSubCol <- expand.grid(subsample = seq(0.5, 0.6, by = 0.01),
								colsample_bytree = 0.7,
								ntrees = 2800,
								min_child_weight = seq(1.5, 2, by = 0.01),
								gamma = seq(0.0, 0.1, by = 0.01))

head(searchGridSubCol)
```

#### 세분화하여 최적화 과정이 시작되면, "RMSE" 최소의 값만 저장하도록 설정
#### progress 를 사용하여, 진행율을 확인
#### Hyper Parameter 탐색과정을 오래 걸리므로, slack과 같은 메신저와 연결하여, 완료되면 메시지 전송을 통해 이원화 작업을 추천

```{r}
if (!require("progress")) { install.packages("progress") }
pb <- progress_bar$new(total = length(searchGridSubCol))

rmseErrorsHyperparameters_xgb <- data.frame()

rmse <- 40000
for (i in 1:nrow(searchGridSubCol)) {
	#Extract Parameters to test
	currentSubsampleRate <- searchGridSubCol[i, 1]
	currentColsampleRate <- searchGridSubCol[i, 2]
	ntreesNum <- searchGridSubCol[i, 3]
	min_child_weight_num <- searchGridSubCol[i, 4]
	gamma_num <- searchGridSubCol[i, 5]

	xgboostModelCV <- xgboost(data = dtrain, nfold = 5, showsd = TRUE,
							metrics = "rmse", verbose = FALSE, "eval_metric" = "rmse",
							"objective" = "reg:linear", "max.depth" = 6, "eta" = 0.01,
							"subsample" = currentSubsampleRate,
							"colsample_bytree" = currentColsampleRate,
							nrounds = ntreesNum, gamma = gamma_num,
							min_child_weight = min_child_weight_num,
							nthread = 8, booster = "gbtree")

	## Predictions
	preds_xgb <- predict(xgboostModelCV, newdata = as.matrix(Validation_XY[, -82]))

	#Save rmse of the last iteration
	rmse1 <- rmse(Validation_XY$SalePrice, preds_xgb)
	rmse = min(rmse, rmse1)

	if (rmse == rmse1) {
		rmseErrorsHyperparameters_xgb_1 <- data.frame(i, rmse1, currentSubsampleRate, currentColsampleRate, ntreesNum, min_child_weight_num, gamma_num)
		colnames(rmseErrorsHyperparameters_xgb_1) <- c("i", "rmse", "currentSubsampleRate", "currentColsampleRate", "ntreesNum", "min_child_weight_num", "gamma_num")
		rmseErrorsHyperparameters_xgb <- rbind(rmseErrorsHyperparameters_xgb, rmseErrorsHyperparameters_xgb_1)
	}
	pb$tick()
}

rmseErrorsHyperparameters_xgb
```

* 결과

```{r}
# i rmse currentSubsampleRate currentColsampleRate ntreesNum min_child_weight_num gamma_num
# 1 21194.13 0.50 0.7 2800 1.50 0.00
# 2 21129.35 0.51 0.7 2800 1.50 0.00
# 6 21102.40 0.55 0.7 2800 1.50 0.00
# 35 21073.59 0.51 0.7 2800 1.53 0.00
# 41 21068.08 0.57 0.7 2800 1.53 0.00
# 46 21043.84 0.51 0.7 2800 1.54 0.00
# 47 20924.94 0.52 0.7 2800 1.54 0.00
# 112 20683.03 0.51 0.7 2800 1.60 0.00
# 1266 20642.76 0.50 0.7 2800 1.63 0.02
# 3258 20629.29 0.51 0.7 2800 1.91 0.05
```

#### "RMSE" 최소값인 Hyper Parameter 값을 불러와서 예측에 실행

```{r}
currentSubsampleRate <- searchGridSubCol[tail(rmseErrorsHyperparameters_xgb$i, 1), 1]
currentColsampleRate <- searchGridSubCol[tail(rmseErrorsHyperparameters_xgb$i, 1), 2]
ntreesNum <- searchGridSubCol[tail(rmseErrorsHyperparameters_xgb$i, 1), 3]
min_child_weight_num <- searchGridSubCol[tail(rmseErrorsHyperparameters_xgb$i, 1), 4]
gamma_num <- searchGridSubCol[tail(rmseErrorsHyperparameters_xgb$i, 1), 5]

Dtrain <- xgb.DMatrix(as.matrix(X_train), label = SalePrice)

bst <- xgboost(data = Dtrain, nfold = 5, showsd = TRUE,
			   metrics = "rmse", verbose = FALSE, "eval_metric" = "rmse",
			   "objective" = "reg:linear", "max.depth" = 6, "eta" = 0.01,
			   "subsample" = currentSubsampleRate,
			   "colsample_bytree" = currentColsampleRate,
			   nrounds = ntreesNum, gamma = gamma_num,
			   min_child_weight = min_child_weight_num,
			   nthread = 8, booster = "gbtree")

### prediction_train all result save
pred <- data.frame(test_id, xgb_pred <- predict(bst, dtest))
colnames(pred) <- c("Id", "SalePrice")

head(pred)
```
